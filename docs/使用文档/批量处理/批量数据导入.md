---
sidebar_position: 2
---

# æ‰¹é‡æ•°æ®å¯¼å…¥

é«˜æ•ˆæ‰¹é‡å¯¼å…¥å„ç§æ ¼å¼çš„æ•°æ®åˆ° FastGPT çŸ¥è¯†åº“ç³»ç»Ÿã€‚

## åŠŸèƒ½æ¦‚è¿°

æ‰¹é‡æ•°æ®å¯¼å…¥åŠŸèƒ½æ”¯æŒä¸€æ¬¡æ€§å¯¼å…¥å¤§é‡æ–‡æ¡£å’Œæ•°æ®ï¼Œè‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢ã€å†…å®¹è§£æå’Œè´¨é‡æ£€æŸ¥ï¼Œå¤§å¹…æå‡çŸ¥è¯†åº“å»ºè®¾æ•ˆç‡ã€‚

## æ”¯æŒçš„æ•°æ®æ ¼å¼

### ğŸ“„ æ–‡æ¡£æ ¼å¼

#### æ–‡æœ¬æ–‡æ¡£
- **PDF æ–‡æ¡£**ï¼š
  - æ”¯æŒæ–‡å­—PDFå’Œæ‰«æPDF
  - è‡ªåŠ¨æå–æ–‡æœ¬ã€å›¾ç‰‡å’Œè¡¨æ ¼
  - ä¿ç•™æ–‡æ¡£ç»“æ„å’Œæ ¼å¼
  - å¤„ç†å¤šåˆ—å¸ƒå±€å’Œå¤æ‚æ’ç‰ˆ

- **Microsoft Office**ï¼š
  - Wordæ–‡æ¡£ï¼ˆ.doc, .docxï¼‰
  - Excelè¡¨æ ¼ï¼ˆ.xls, .xlsxï¼‰
  - PowerPointæ¼”ç¤ºæ–‡ç¨¿ï¼ˆ.ppt, .pptxï¼‰

- **çº¯æ–‡æœ¬æ ¼å¼**ï¼š
  - TXTæ–‡ä»¶
  - RTFå¯Œæ–‡æœ¬æ ¼å¼
  - Markdownæ–‡æ¡£ï¼ˆ.mdï¼‰

#### ç»“æ„åŒ–æ•°æ®
- **JSON æ ¼å¼**ï¼š
  ```json
  {
    "documents": [
      {
        "title": "æ–‡æ¡£æ ‡é¢˜",
        "content": "æ–‡æ¡£å†…å®¹",
        "category": "åˆ†ç±»",
        "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"]
      }
    ]
  }
  ```

- **CSV æ ¼å¼**ï¼š
  ```csv
  æ ‡é¢˜,å†…å®¹,åˆ†ç±»,æ ‡ç­¾
  "APIä½¿ç”¨æŒ‡å—","è¯¦ç»†çš„APIä½¿ç”¨è¯´æ˜...","æŠ€æœ¯æ–‡æ¡£","API,æ•™ç¨‹"
  ```

- **XML æ ¼å¼**ï¼š
  ```xml
  <documents>
    <document>
      <title>æ–‡æ¡£æ ‡é¢˜</title>
      <content>æ–‡æ¡£å†…å®¹</content>
      <category>åˆ†ç±»</category>
    </document>
  </documents>
  ```

### ğŸŒ ç½‘é¡µæ ¼å¼

#### Web å†…å®¹
- **HTML æ–‡ä»¶**ï¼š
  - è‡ªåŠ¨æå–æ­£æ–‡å†…å®¹
  - è¿‡æ»¤å¯¼èˆªå’Œå¹¿å‘Š
  - ä¿ç•™æ–‡æœ¬ç»“æ„

- **ç½‘é¡µå¿«ç…§**ï¼š
  - MHTMLæ ¼å¼
  - å®Œæ•´é¡µé¢å†…å®¹
  - åŒ…å«æ ·å¼å’Œå›¾ç‰‡

### ğŸ—„ï¸ æ•°æ®åº“å¯¼å…¥

#### å…³ç³»å‹æ•°æ®åº“
- **MySQL**
- **PostgreSQL**
- **SQL Server**
- **Oracle**

#### NoSQL æ•°æ®åº“
- **MongoDB**
- **Elasticsearch**
- **Redis**

#### è¿æ¥é…ç½®ç¤ºä¾‹
```json
{
  "type": "mysql",
  "host": "localhost",
  "port": 3306,
  "database": "knowledge_base",
  "username": "user",
  "password": "password",
  "table": "documents",
  "fields": {
    "title": "title",
    "content": "content",
    "category": "category"
  }
}
```

## å¯¼å…¥æµç¨‹

### 1. æ•°æ®å‡†å¤‡

#### æ–‡ä»¶ç»„ç»‡
```
å¯¼å…¥æ•°æ®/
â”œâ”€â”€ æŠ€æœ¯æ–‡æ¡£/
â”‚   â”œâ”€â”€ APIæ–‡æ¡£.pdf
â”‚   â”œâ”€â”€ å¼€å‘æŒ‡å—.docx
â”‚   â””â”€â”€ å¸¸è§é—®é¢˜.md
â”œâ”€â”€ äº§å“èµ„æ–™/
â”‚   â”œâ”€â”€ äº§å“ä»‹ç».pptx
â”‚   â”œâ”€â”€ åŠŸèƒ½è¯´æ˜.pdf
â”‚   â””â”€â”€ ç”¨æˆ·æ‰‹å†Œ.docx
â””â”€â”€ é…ç½®æ–‡ä»¶/
    â”œâ”€â”€ import_config.json
    â””â”€â”€ field_mapping.csv
```

#### é…ç½®æ–‡ä»¶è®¾ç½®
```json
{
  "import_settings": {
    "batch_size": 100,
    "parallel_workers": 5,
    "skip_duplicates": true,
    "auto_categorize": true
  },
  "content_processing": {
    "extract_images": true,
    "preserve_formatting": true,
    "auto_segment": true,
    "min_content_length": 50
  },
  "quality_check": {
    "enable_validation": true,
    "check_encoding": true,
    "remove_duplicates": true
  }
}
```

### 2. æ‰§è¡Œå¯¼å…¥

#### 2.1 é€‰æ‹©å¯¼å…¥æ–¹å¼

**æ–‡ä»¶ä¸Šä¼ å¯¼å…¥**
1. è¿›å…¥æ‰¹é‡å¤„ç†é¡µé¢
2. é€‰æ‹©"æ‰¹é‡æ•°æ®å¯¼å…¥"
3. ä¸Šä¼ æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
4. é…ç½®å¯¼å…¥å‚æ•°
5. å¼€å§‹å¯¼å…¥ä»»åŠ¡

**URLå¯¼å…¥**
1. æä¾›æ•°æ®æºURL
2. é…ç½®è®¿é—®è®¤è¯
3. è®¾ç½®æŠ“å–è§„åˆ™
4. æ‰§è¡Œè¿œç¨‹å¯¼å…¥

**æ•°æ®åº“å¯¼å…¥**
1. é…ç½®æ•°æ®åº“è¿æ¥
2. è®¾ç½®æŸ¥è¯¢è¯­å¥
3. æ˜ å°„å­—æ®µå…³ç³»
4. æ‰§è¡Œæ‰¹é‡å¯¼å…¥

#### 2.2 å¯¼å…¥ç›‘æ§

```mermaid
graph LR
    A[å¼€å§‹å¯¼å…¥] --> B[æ•°æ®é¢„å¤„ç†]
    B --> C[æ ¼å¼è¯†åˆ«]
    C --> D[å†…å®¹è§£æ]
    D --> E[è´¨é‡æ£€æŸ¥]
    E --> F[å­˜å‚¨å…¥åº“]
    F --> G[å¯¼å…¥å®Œæˆ]
    
    H[è¿›åº¦ç›‘æ§] --> A
    H --> B
    H --> C
    H --> D
    H --> E
    H --> F
```

**ç›‘æ§ç•Œé¢æ˜¾ç¤ºï¼š**
- æ€»ä»»åŠ¡æ•°é‡å’Œè¿›åº¦
- å½“å‰å¤„ç†æ–‡ä»¶å
- æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
- é”™è¯¯æ—¥å¿—è¯¦æƒ…
- é¢„è®¡å®Œæˆæ—¶é—´

### 3. ç»“æœéªŒè¯

#### 3.1 å¯¼å…¥æŠ¥å‘Š
```
å¯¼å…¥ä»»åŠ¡æŠ¥å‘Š
==============
ä»»åŠ¡ID: BATCH_20241201_001
å¼€å§‹æ—¶é—´: 2024-12-01 10:00:00
ç»“æŸæ—¶é—´: 2024-12-01 10:45:30
æ€»è€—æ—¶: 45åˆ†30ç§’

ç»Ÿè®¡ä¿¡æ¯:
- æ€»æ–‡ä»¶æ•°: 1,500
- æˆåŠŸå¯¼å…¥: 1,485
- å¯¼å…¥å¤±è´¥: 15
- è·³è¿‡é‡å¤: 0

è¯¦ç»†ç»“æœ:
- PDFæ–‡æ¡£: 800ä¸ª (æˆåŠŸ: 795, å¤±è´¥: 5)
- Wordæ–‡æ¡£: 500ä¸ª (æˆåŠŸ: 495, å¤±è´¥: 5)
- Excelæ–‡æ¡£: 200ä¸ª (æˆåŠŸ: 195, å¤±è´¥: 5)
```

#### 3.2 è´¨é‡æ£€æŸ¥
- **å†…å®¹å®Œæ•´æ€§**ï¼šæ£€æŸ¥å¯¼å…¥å†…å®¹æ˜¯å¦å®Œæ•´
- **æ ¼å¼æ­£ç¡®æ€§**ï¼šéªŒè¯æ ¼å¼è½¬æ¢æ˜¯å¦æ­£ç¡®
- **é‡å¤æ£€æµ‹**ï¼šè¯†åˆ«é‡å¤æˆ–ç›¸ä¼¼å†…å®¹
- **åˆ†ç±»å‡†ç¡®æ€§**ï¼šéªŒè¯è‡ªåŠ¨åˆ†ç±»ç»“æœ

## é«˜çº§åŠŸèƒ½

### 1. æ™ºèƒ½é¢„å¤„ç†

#### å†…å®¹æ¸…æ´—
```python
def clean_content(text):
    """æ™ºèƒ½å†…å®¹æ¸…æ´—"""
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    
    # ä¿®å¤ç¼–ç é—®é¢˜
    text = fix_encoding(text)
    
    # æ ‡å‡†åŒ–æ ¼å¼
    text = normalize_format(text)
    
    # ç§»é™¤æ•æ„Ÿä¿¡æ¯
    text = remove_sensitive_data(text)
    
    return text
```

#### è‡ªåŠ¨åˆ†æ®µ
```python
def auto_segment(content, max_length=1000):
    """æ™ºèƒ½å†…å®¹åˆ†æ®µ"""
    segments = []
    
    # æŒ‰ç« èŠ‚åˆ†å‰²
    chapters = split_by_headers(content)
    
    for chapter in chapters:
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = split_by_paragraphs(chapter)
        
        # åˆå¹¶çŸ­æ®µè½
        merged = merge_short_paragraphs(paragraphs, max_length)
        segments.extend(merged)
    
    return segments
```

### 2. æ‰¹é‡è½¬æ¢

#### æ ¼å¼æ ‡å‡†åŒ–
- **æ–‡æ¡£æ ¼å¼ç»Ÿä¸€**ï¼šè½¬æ¢ä¸ºç»Ÿä¸€çš„å†…éƒ¨æ ¼å¼
- **ç¼–ç æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€å­—ç¬¦ç¼–ç ï¼ˆUTF-8ï¼‰
- **ç»“æ„è§„èŒƒåŒ–**ï¼šæ ‡å‡†åŒ–æ–‡æ¡£ç»“æ„å’Œå±‚çº§

#### å†…å®¹å¢å¼º
```python
def enhance_content(document):
    """å†…å®¹å¢å¼ºå¤„ç†"""
    # è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦
    document['summary'] = generate_summary(document['content'])
    
    # æå–å…³é”®è¯
    document['keywords'] = extract_keywords(document['content'])
    
    # è‡ªåŠ¨åˆ†ç±»
    document['category'] = auto_categorize(document['content'])
    
    # ç”Ÿæˆæ ‡ç­¾
    document['tags'] = generate_tags(document['content'])
    
    return document
```

### 3. å¢é‡æ›´æ–°

#### å˜æ›´æ£€æµ‹
```python
def detect_changes(new_content, existing_content):
    """æ£€æµ‹å†…å®¹å˜æ›´"""
    changes = {
        'added': [],
        'modified': [],
        'deleted': []
    }
    
    # è®¡ç®—å†…å®¹å“ˆå¸Œ
    new_hash = calculate_hash(new_content)
    existing_hash = calculate_hash(existing_content)
    
    if new_hash != existing_hash:
        changes['modified'].append({
            'content_id': content_id,
            'change_type': 'content_update',
            'timestamp': datetime.now()
        })
    
    return changes
```

#### æ™ºèƒ½åˆå¹¶
- **ç‰ˆæœ¬æ§åˆ¶**ï¼šä¿ç•™å†å²ç‰ˆæœ¬
- **å†²çªè§£å†³**ï¼šæ™ºèƒ½å¤„ç†å†…å®¹å†²çª
- **å¢é‡åŒæ­¥**ï¼šåªæ›´æ–°å˜æ›´éƒ¨åˆ†

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯ç±»å‹

#### 1. æ ¼å¼é”™è¯¯
```python
class FormatError(Exception):
    """æ ¼å¼é”™è¯¯å¼‚å¸¸"""
    def __init__(self, file_path, error_detail):
        self.file_path = file_path
        self.error_detail = error_detail
        super().__init__(f"æ ¼å¼é”™è¯¯ {file_path}: {error_detail}")

# é”™è¯¯å¤„ç†ç¤ºä¾‹
try:
    content = parse_document(file_path)
except FormatError as e:
    log_error(e)
    # å°è¯•å…¶ä»–è§£ææ–¹å¼
    content = fallback_parse(file_path)
```

#### 2. ç¼–ç é”™è¯¯
```python
def handle_encoding_error(file_path):
    """å¤„ç†ç¼–ç é”™è¯¯"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'big5']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return f.read()
        except UnicodeDecodeError:
            continue
    
    raise EncodingError(f"æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç : {file_path}")
```

#### 3. å¤§æ–‡ä»¶å¤„ç†
```python
def process_large_file(file_path, chunk_size=10*1024*1024):
    """å¤„ç†å¤§æ–‡ä»¶"""
    file_size = os.path.getsize(file_path)
    
    if file_size > 100*1024*1024:  # 100MB
        # åˆ†å—å¤„ç†
        return process_in_chunks(file_path, chunk_size)
    else:
        # ç›´æ¥å¤„ç†
        return process_directly(file_path)
```

### é”™è¯¯æ¢å¤æœºåˆ¶

#### æ–­ç‚¹ç»­ä¼ 
```python
def resume_import(task_id):
    """æ¢å¤ä¸­æ–­çš„å¯¼å…¥ä»»åŠ¡"""
    # åŠ è½½ä»»åŠ¡çŠ¶æ€
    task_state = load_task_state(task_id)
    
    # è·å–å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨
    processed_files = task_state['processed_files']
    
    # ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶
    remaining_files = get_remaining_files(task_state['file_list'], processed_files)
    
    # æ¢å¤å¤„ç†
    continue_processing(remaining_files, task_state)
```

#### é‡è¯•æœºåˆ¶
```python
def import_with_retry(file_path, max_retries=3):
    """å¸¦é‡è¯•çš„å¯¼å…¥å¤„ç†"""
    for attempt in range(max_retries):
        try:
            return import_file(file_path)
        except RetryableError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                time.sleep(wait_time)
                continue
            else:
                raise e
```

## æ€§èƒ½ä¼˜åŒ–

### å¹¶è¡Œå¤„ç†ç­–ç•¥

#### å¤šçº¿ç¨‹å¤„ç†
```python
from concurrent.futures import ThreadPoolExecutor
import queue

def parallel_import(file_list, max_workers=5):
    """å¹¶è¡Œå¯¼å…¥æ–‡ä»¶"""
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤ä»»åŠ¡
        futures = {
            executor.submit(import_file, file_path): file_path 
            for file_path in file_list
        }
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(futures):
            file_path = futures[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                log_error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    return results
```

#### å†…å­˜ç®¡ç†
```python
def memory_efficient_import(large_file_list):
    """å†…å­˜å‹å¥½çš„æ‰¹é‡å¯¼å…¥"""
    batch_size = calculate_optimal_batch_size()
    
    for i in range(0, len(large_file_list), batch_size):
        batch = large_file_list[i:i + batch_size]
        
        # å¤„ç†æ‰¹æ¬¡
        process_batch(batch)
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        
        # ç›‘æ§å†…å­˜ä½¿ç”¨
        if get_memory_usage() > 80:  # 80%
            batch_size = max(1, batch_size // 2)
```

### ç¼“å­˜ä¼˜åŒ–

#### è§£æç»“æœç¼“å­˜
```python
import functools

@functools.lru_cache(maxsize=1000)
def parse_document_cached(file_path, file_hash):
    """ç¼“å­˜æ–‡æ¡£è§£æç»“æœ"""
    return parse_document(file_path)

def import_with_cache(file_path):
    """ä½¿ç”¨ç¼“å­˜çš„å¯¼å…¥"""
    file_hash = calculate_file_hash(file_path)
    return parse_document_cached(file_path, file_hash)
```

## è´¨é‡ä¿è¯

### æ•°æ®éªŒè¯

#### å†…å®¹è´¨é‡æ£€æŸ¥
```python
def validate_content_quality(content):
    """å†…å®¹è´¨é‡éªŒè¯"""
    issues = []
    
    # æ£€æŸ¥å†…å®¹é•¿åº¦
    if len(content.strip()) < 10:
        issues.append("å†…å®¹è¿‡çŸ­")
    
    # æ£€æŸ¥å­—ç¬¦ç¼–ç 
    if not is_valid_encoding(content):
        issues.append("ç¼–ç é—®é¢˜")
    
    # æ£€æŸ¥å†…å®¹ç»“æ„
    if not has_valid_structure(content):
        issues.append("ç»“æ„ä¸è§„èŒƒ")
    
    # æ£€æŸ¥é‡å¤å†…å®¹
    if is_duplicate_content(content):
        issues.append("é‡å¤å†…å®¹")
    
    return issues
```

#### è‡ªåŠ¨ä¿®å¤
```python
def auto_fix_content(content, issues):
    """è‡ªåŠ¨ä¿®å¤å†…å®¹é—®é¢˜"""
    for issue in issues:
        if issue == "ç¼–ç é—®é¢˜":
            content = fix_encoding(content)
        elif issue == "ç»“æ„ä¸è§„èŒƒ":
            content = normalize_structure(content)
        elif issue == "æ ¼å¼é”™è¯¯":
            content = fix_formatting(content)
    
    return content
```

### å¯¼å…¥åéªŒè¯

#### å®Œæ•´æ€§æ£€æŸ¥
```python
def verify_import_completeness(original_files, imported_records):
    """éªŒè¯å¯¼å…¥å®Œæ•´æ€§"""
    verification_report = {
        'total_files': len(original_files),
        'imported_count': len(imported_records),
        'missing_files': [],
        'corrupt_imports': []
    }
    
    # æ£€æŸ¥æ–‡ä»¶å¯¹åº”å…³ç³»
    for file_path in original_files:
        if not find_imported_record(file_path, imported_records):
            verification_report['missing_files'].append(file_path)
    
    # æ£€æŸ¥å¯¼å…¥è´¨é‡
    for record in imported_records:
        if not validate_record_integrity(record):
            verification_report['corrupt_imports'].append(record)
    
    return verification_report
```

## æœ€ä½³å®è·µ

### 1. å¯¼å…¥å‰å‡†å¤‡
- **æ•°æ®æ•´ç†**ï¼šç»Ÿä¸€æ–‡ä»¶æ ¼å¼å’Œå‘½åè§„èŒƒ
- **è´¨é‡é¢„æ£€**ï¼šæå‰æ£€æŸ¥æ•°æ®è´¨é‡é—®é¢˜
- **å¤‡ä»½åŸå§‹æ•°æ®**ï¼šä¿ç•™åŸå§‹æ•°æ®å‰¯æœ¬
- **æµ‹è¯•å°æ‰¹é‡**ï¼šå…ˆç”¨å°æ•°æ®é›†æµ‹è¯•æµç¨‹

### 2. å¯¼å…¥é…ç½®ä¼˜åŒ–
- **åˆç†è®¾ç½®æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ®ç³»ç»Ÿæ€§èƒ½è°ƒæ•´
- **é€‰æ‹©é€‚å½“å¹¶å‘æ•°**ï¼šé¿å…èµ„æºè¿‡è½½
- **å¯ç”¨é”™è¯¯æ¢å¤**ï¼šç¡®ä¿ä»»åŠ¡å¯æ¢å¤
- **é…ç½®ç›‘æ§å‘Šè­¦**ï¼šåŠæ—¶å‘ç°é—®é¢˜

### 3. å¯¼å…¥åç»´æŠ¤
- **å®šæœŸè´¨é‡æ£€æŸ¥**ï¼šæ£€æŸ¥å¯¼å…¥æ•°æ®è´¨é‡
- **æ›´æ–°ç´¢å¼•**ï¼šé‡å»ºæœç´¢ç´¢å¼•
- **æ€§èƒ½ç›‘æ§**ï¼šç›‘æ§ç³»ç»Ÿæ€§èƒ½å˜åŒ–
- **ç”¨æˆ·åé¦ˆæ”¶é›†**ï¼šæ”¶é›†ä½¿ç”¨åé¦ˆ

---

*é€šè¿‡æ‰¹é‡æ•°æ®å¯¼å…¥åŠŸèƒ½ï¼Œå¯ä»¥å¿«é€Ÿæ„å»ºå¤§è§„æ¨¡çŸ¥è¯†åº“ï¼Œæå‡æ•°æ®å¤„ç†æ•ˆç‡ã€‚*
